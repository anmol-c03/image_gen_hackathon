{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_image import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using T2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from T2I import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "# replace prompt as required by reference image\n",
    "\n",
    "prompt=[\"Beautiful, carpet, 4k picture, high quality, color gradient rainbow , make it dim , similar to input image\",\n",
    "       \"pattern,orange,red,yellow,purple,blue,artistic,floral,lace,motifs\",\n",
    "        \"design,abstract,rich,dynamic,swirls,deep,vivid,decorative,warm,complex\",\n",
    "        \"intricate,colorful,ornate,paisley,floral,detailed,swirling,vibrant,textured,elegant\",\n",
    "        \"motifs,artistic,pattern,deep,warm,vivid,intricate,ornate,colorful,dynamic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=5 # keep steps same as number of prompts and the image you want to generate\n",
    "for i in range(steps):\n",
    "  gen_images = pipe(\n",
    "    prompt=prompt[i],\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7,\n",
    "    adapter_conditioning_scale=0.9,\n",
    "    adapter_conditioning_factor=1\n",
    "  ).images[0]\n",
    "  gen_images.save(f'images/out_carpet{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference using IP adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://designcompetition.explorug.online/images/Border/2/16.webp\")\n",
    "ip_image = load_image(\"https://designcompetition.explorug.online/images/Border/2/17.webp\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(4)\n",
    "images_l=[]\n",
    "for _ in range(5):\n",
    "    images = pipeline(\n",
    "        prompt=\"best quality, high quality\",\n",
    "        image=image,\n",
    "        ip_adapter_image=ip_image,\n",
    "        generator=generator,\n",
    "        strength=0.6,\n",
    "    ).images[0]\n",
    "    images_l.append(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer using stable_diffusion with clip\n",
    "CLIP interogator generates prompts for the input/reference image , that prompt is passed through runway/stable_diffusion to generate new images that resembles original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/path/to/your/images\"  # desc.csv is saved in this folder_path, so make give read and write access\n",
    "prompt_mode = 'best' \n",
    "output_mode = 'desc.csv' \n",
    "max_filename_len = 128 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "caption_model_name = 'blip-large' \n",
    "clip_model_name = 'ViT-L-14/openai' \n",
    "\n",
    "config = Config()\n",
    "config.clip_model_name = clip_model_name\n",
    "config.caption_model_name = caption_model_name\n",
    "ci = Interrogator(config)\n",
    "\n",
    "ci.config.quiet = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt(folder_path,prompt_mode,output_mode,max_filename_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"/path/to/your/csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mejan\n",
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.load_lora_weights(\"/content/pytorch_lora_weights.safetensors\")\n",
    "\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize refrence image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(df.shape[0]):\n",
    "image=df['image'][44]\n",
    "prompt=df['prompt'][44]\n",
    "full_image_path=os.path.join(folder_path,image)\n",
    "init_image = Image.open(full_image_path).convert(\"RGB\")  # Provide path to your image\n",
    "init_image = init_image.resize((512, 512))  # Resize to 512x512 if necessary\n",
    "init_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_prompt = \"blurry, low quality, distorted, overexposed, watermark\"\n",
    "\n",
    "strength = 0.7  # How much the new image differs from the original (0 = no change, 1 = fully new)\n",
    "guidance_scale = [4,4.5,5,5.5,6]  # Controls how strongly the model follows the text prompt\n",
    "\n",
    "for k in range(len(guidance_scale)):\n",
    "    output_image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=init_image,\n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale[k]\n",
    "    ).images[0]\n",
    "    os.makedirs(image,exist_ok=True)\n",
    "    output_image.save(f\"{image}/output_image{k}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
