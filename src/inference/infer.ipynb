{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_image import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using T2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from T2I import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "# replace prompt as required by reference image\n",
    "\n",
    "prompt=[\"Beautiful, carpet, 4k picture, high quality, color gradient rainbow , make it dim , similar to input image\",\n",
    "       \"pattern,orange,red,yellow,purple,blue,artistic,floral,lace,motifs\",\n",
    "        \"design,abstract,rich,dynamic,swirls,deep,vivid,decorative,warm,complex\",\n",
    "        \"intricate,colorful,ornate,paisley,floral,detailed,swirling,vibrant,textured,elegant\",\n",
    "        \"motifs,artistic,pattern,deep,warm,vivid,intricate,ornate,colorful,dynamic\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=5 # keep steps same as number of prompts and the image you want to generate\n",
    "for i in range(steps):\n",
    "  gen_images = pipe(\n",
    "    prompt=prompt[i],\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7,\n",
    "    adapter_conditioning_scale=0.9,\n",
    "    adapter_conditioning_factor=1\n",
    "  ).images[0]\n",
    "  gen_images.save(f'images/out_carpet{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference using IP adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://designcompetition.explorug.online/images/Border/2/16.webp\")\n",
    "ip_image = load_image(\"https://designcompetition.explorug.online/images/Border/2/17.webp\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(4)\n",
    "images_l=[]\n",
    "for _ in range(5):\n",
    "    images = pipeline(\n",
    "        prompt=\"best quality, high quality\",\n",
    "        image=image,\n",
    "        ip_adapter_image=ip_image,\n",
    "        generator=generator,\n",
    "        strength=0.6,\n",
    "    ).images[0]\n",
    "    images_l.append(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer using stable_diffusion with clip\n",
    "CLIP interogator generates prompts for the input/reference image , that prompt is passed through runway/stable_diffusion to generate new images that resembles original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import *\n",
    "from stability import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/content\" #@param {type:\"string\"}\n",
    "prompt_mode = 'best' #@param [\"best\",\"fast\",\"classic\",\"negative\"]\n",
    "output_mode = 'desc.csv' #@param [\"desc.csv\",\"rename\"]\n",
    "max_filename_len = 128 #@param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "caption_model_name = 'blip-large' #@param [\"blip-base\", \"blip-large\", \"git-large-coco\"]\n",
    "clip_model_name = 'ViT-L-14/openai' #@param [\"ViT-L-14/openai\", \"ViT-H-14/laion2b_s32b_b79k\"]\n",
    "\n",
    "config = Config()\n",
    "config.clip_model_name = clip_model_name\n",
    "config.caption_model_name = caption_model_name\n",
    "ci = Interrogator(config)\n",
    "\n",
    "ci.config.quiet = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompt(folder_path,prompt_mode,output_mode,max_filename_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
